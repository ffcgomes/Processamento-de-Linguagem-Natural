{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "house-web-scrapper-ceara-rede-imoveis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihr3Qs6RtoR6"
      },
      "source": [
        "#A house web scraper in Python!\n",
        "In a few months I’ll have to leave my rented apartment and look for a new one. As painful as this experience can be, especially as a real estate bubble looms in the horizon, I decided to use it as yet another incentive to improve my Python skills! In the end I want to be able to do two things:\n",
        "· Scrape all the search results from one of the main real estate websites in Portugal (where I live) and build a database with all the listings found\n",
        "· Use the collected listings to perform some EDA, and ultimately try to find undervalued properties\n",
        "The website I will be scraping is the real estate portal from Sapo, one of the oldest and most visited websites in Portugal. They have a very large amount of real estate listings for us to scrape. Chances are you are using a different website, but you should be able to adapt the code very easily.\n",
        "Before we begin with the code snippets, let me just give you a summary of what I will be doing. I will use the results page from a simple search in Sapo website where I can specify some parameters beforehand (like zone, price filters, number of rooms, etc) to reduce the task time, or simply query the whole list of results in Lisbon.\n",
        "We then need to use a command to reach ask a response from the website. The result will be some html code, which we will then use to get the elements we want for our final table. After deciding what to take from each search result property, we need a for loop to open each of the search pages and perform the scraping.\n",
        "That sounds pretty easy, where do I start?\n",
        "Like most projects, we need to import the modules to be used. I will use Beautiful Soup to take care of the html’s we will be fetching. Always make sure the site you are trying to access allows scraping. You can easily do that if you add “/robots.txt” to the original domain. Inside this file you can see if there are guidelines regarding what is allowed to scrape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsCgF_QatcvI"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09-G_X-rue_h"
      },
      "source": [
        "Some websites automatically block any kind of scraping, and that’s why I’ll define a header to pass along the get command, which will basically make our queries to the website look like they are coming from an actual browser. When we run the program, I’ll have a sleep command between pages, so we can mimic a “more human” behavior and don’t overload the site with several requests per second. You will get blocked if you scrape too aggressively, so it’s a nice policy to be polite while scraping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aixj72QRufkK"
      },
      "source": [
        "headers = ({'User-Agent':\n",
        "            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCyyovO7u2wW"
      },
      "source": [
        "Then we define the base url to be used when querying the website. For this purpose I will just limit my search to Lisbon and sort by creation date. The address bar quickly updates and gives me the parameters sa=11 for Lisbon, and or=10 for the sorting, which I will use in the sapo variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXOxiQi-u3Z2"
      },
      "source": [
        "sapo = \"https://www.cri-ce.com.br/imoveis/a-venda/casa/itaitinga\"\n",
        "response = get(sapo, headers=headers)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EY6FVwC335D"
      },
      "source": [
        "And now we can test if we can communicate with the website. You can get several codes from this command, but if you get “200” it’s usually a sign that you’re good to go. You can see a list of these codes here.\n",
        "We can print the response and the first 1000 characters of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNkP8WYw359W",
        "outputId": "ff70ac15-c46d-45b2-892c-26d99b9c10d1"
      },
      "source": [
        "print(response)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Response [200]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc6XYE1f4AS7",
        "outputId": "d30839ae-7c4d-4a5c-fce7-dd59631b8246"
      },
      "source": [
        "print(response.text[:1000])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html><html lang=\"pt-BR\" style=\"--primary-color: 246, 144, 49; --primary-color-light: 249, 223, 199; --primary-color-medium: 248, 170, 98; --primary-color-dark: 176, 94, 17; --secondary-color: 84, 84, 84; --secondary-color-light: 161, 161, 161; --secondary-color-medium: 110, 110, 110; --secondary-color-dark: 33, 33, 33; --text-primary: #3d3d3d; --text-secondary: #FFF\"><head><!--M^s0-2 s0 2--><link rel=\"icon\" type=\"image/x-icon\" href=\"https://imgs.kenlo.io/VWRCUkQ2Tnp3d1BJRDBJVe1s0xgxS7daNJUEv7tewTj5teT1Ozqgzm1JNjAvUFRCJadQk2NyQ4sn9UZultlp41E0iI0WVb63pyib08LPuonI8wO937X4npyd++rBfez57sdijPeqSgH9uvU-F9J8OhwjPLd2GBXgVomMYyAP+GOH+gDHS7xMCS4fxktgz09F.png\"><meta charset=\"utf-8\"><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"><meta name=\"description\" content=\"Na Ceará Rede Imóveis você encontra casas à venda em Ancuri, Barrocão, Jabuti, Gereraú e Pedras - Itaitinga. Confira mais imóveis à venda ou para alugar em \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_BMu1LV4iGZ"
      },
      "source": [
        "Alright, we’re all set to start exploring whatever we get from the website. We need to define the Beautiful Soup object that will help us read this html. That’s what BS does: it picks the text from the response and parses the information in a way that makes it easier for you to navigate in its structure and get its contents.\n",
        "#Time to make some Soup!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3NQMFuU4lXt"
      },
      "source": [
        "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "#html_soup"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m78ccPEY6yHx"
      },
      "source": [
        "Before extracting the price, we want to be able to identify each result in the page. In order to know what tags we need to call, we can follow them from the price tag to the top until we reach something that looks like the main container for each result. We can see it below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MkTY2al7RfO"
      },
      "source": [
        "house_containers = html_soup.find_all('div', class_=\"link-all\")\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N46jqJWb7il5"
      },
      "source": [
        "We now have an object that can be iterated while we scrape the results in each search page. Let’s try and get the price we saw before. I’ll define the variable first which will be the structure of our first house (picked up from the house_containers variable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACs-JZe_7jNC"
      },
      "source": [
        "first = house_containers[0]\n",
        "first.find_all('span')\n",
        "print(first.prettify())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC85kIst8Js1"
      },
      "source": [
        "So the price is quite easily obtainable, but there are some special characters along the text. A simple way to take care of it is to simply replace the special character with nothing. I’ll break it down below, as I transform the string into an integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fE3Ea9LY8KON",
        "outputId": "291a8dca-db56-4a11-8120-cabc06391826"
      },
      "source": [
        "var_1 = first.find_all('span')[0].text\n",
        "var_1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'R$ 40.000'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZxnIk7z9ZGV",
        "outputId": "bb575ece-4377-4545-af5f-16f8fdf26480"
      },
      "source": [
        "var_1=var_1.replace('.','')\n",
        "var_1 = float(var_1[3:])\n",
        "print(var_1,type(var_1))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000.0 <class 'float'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9fKpjtG-rMN"
      },
      "source": [
        "In this last step, itertools helped me retrieve only the digits from the second step. We just scraped our first price! The other fields we want to get are: Title, Size, Date posted, location, condition status, short description, link for the property and link to a thumbnail.\n",
        "I’ll give some examples below before we build the amazing for loop that will get us every result from every page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EKt7kkQF-to8",
        "outputId": "216ed751-ac49-4a9d-a0cb-385994964bcd"
      },
      "source": [
        "bairro = first.find_all('h2',class_='card-title')[0].text\n",
        "bairro"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ancuri'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UXZfYuDu-4f1",
        "outputId": "e923d8a3-5fa9-475b-d343-f8f8eab1e3f5"
      },
      "source": [
        "titulo = first.find_all('h3',class_='card-text')[0].text\n",
        "titulo"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Casa em Itaitinga'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "CeVE0D4p_DLx",
        "outputId": "20d100de-b3b2-4180-8747-7b825f2cf664"
      },
      "source": [
        "descricao = first.find_all('p',class_='description')[0].text\n",
        "descricao"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Casa REPASSE\\r\\n 02 Quartos (02 suítes )\\r\\n Wc social  reversível \\r\\n Sala Ampla\\r\\n Cozinha americana\\r\\n Área de serviço coberta \\r\\n quintal e varanda \\r\\n 02 Vagas na Garagem;\\r\\n 75 m² área construída.\\r\\n\\r\\nCorretores de Plantão  \\r\\n Segunda a sábado: 8:00 as 17:00 \\r\\n Domingo: 8:00 as 13:00.\\r\\n\\r\\n Ciro Chaves Imóveis  18769J\\r\\n (85) 99227-8053 Whatsapp'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjrQE4MK_fVX",
        "outputId": "9f9b913e-a438-4da9-a4be-43a9b3c6a689"
      },
      "source": [
        "items = first.find_all(class_=\"values\")[0]\n",
        "items.prettify()\n",
        "items = items.find_all('span',class_='h-money')\n",
        "dados_items={'Quartos':0,'Suítes':0,'Banheiros':0,'Vagas':0,'Área':0}\n",
        "for i in range(5):\n",
        "  items[i]\n",
        "  item = float([d for d in items[i]][0])\n",
        "  #print(item)\n",
        "  if i==0:  \n",
        "    dados_items.update({'Quartos':item})\n",
        "  elif i==1:\n",
        "    dados_items.update({'Suítes':item})\n",
        "  elif i==2:\n",
        "    dados_items.update({'Banheiros':item})\n",
        "  elif i==3:\n",
        "    dados_items.update({'Vagas':item})\n",
        "  elif i==4:\n",
        "    dados_items.update({'Área':item})\n",
        "\n",
        "dados_items"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Banheiros': 2.0, 'Quartos': 2.0, 'Suítes': 2.0, 'Vagas': 2.0, 'Área': 75.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "hDrLPs37_P6n",
        "outputId": "c9aae777-7e97-4aa3-d947-780f41940e1d"
      },
      "source": [
        "items = pd.DataFrame(dados_items.items(),columns=['Item','Valor'])\n",
        "items"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Item</th>\n",
              "      <th>Valor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Quartos</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Suítes</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Banheiros</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Vagas</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Área</td>\n",
              "      <td>75.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Item  Valor\n",
              "0    Quartos    2.0\n",
              "1     Suítes    2.0\n",
              "2  Banheiros    2.0\n",
              "3      Vagas    2.0\n",
              "4       Área   75.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "r03-3b9n_eKI",
        "outputId": "94eeef90-14ec-46c9-af7d-be03f0763e93"
      },
      "source": [
        "first.find_all('p',cclass_='searchPropertyDescription')[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d7425f059cca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'searchPropertyDescription'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fle_2i4p__Qt"
      },
      "source": [
        "#gets all the links\n",
        "for url in first.find_all('a'):\n",
        "  print(url.get('href'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJEHJF5cAnEc"
      },
      "source": [
        "#Enough with tags, let’s scrape some pages already!\n",
        "Once your’re comfortable with the fields to extract and you found a way to extract them all from each result container, it’s time to setup the base of our crawler. The following lists will be created to handle our data and later be used to put together the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiPg75MvApOt"
      },
      "source": [
        "# setting up the lists that will form our dataframe with all the results\n",
        "titles = []\n",
        "created = []\n",
        "prices = []\n",
        "areas = []\n",
        "zone = []\n",
        "condition = []\n",
        "descriptions = []\n",
        "urls = []\n",
        "thumbnails = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTnmzRf7B9Dr"
      },
      "source": [
        "From a quick check on the original web page, I see there are 871 pages of results. We can give it a little more room and set the loop for 900 iterations. We’ll add something to break the loop if it finds a page without any house container. The page command is the &pn=x in the end of the address, where x is the results page number.\n",
        "The code is made up with two for loops, which navigate through every house in the page, for every page possible.\n",
        "If you follow along, you can notice we’re simply collecting the data we already explored above as we cycle through results. The price field turned out more complicated as there were cases containing both Sell and Rent prices separated by a “/”. In some results, the index 2 returned “Contacte Anunciante” so I had to update the code with an if statement to look for the price in the next index position."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0qZISFTB5fh"
      },
      "source": [
        "%%time\n",
        "\n",
        "n_pages = 0\n",
        "\n",
        "for page in range(0,2100):\n",
        "    n_pages += 1\n",
        "    sapo_url = 'https://casa.sapo.pt/Venda/Apartamentos/?sa=11&lp=10000&or=10'+'&pn='+str(page)\n",
        "    r = get(sapo_url, headers=headers)\n",
        "    page_html = BeautifulSoup(r.text, 'html.parser')\n",
        "    house_containers = page_html.find_all('div', class_=\"searchResultProperty\")\n",
        "    if house_containers != []:\n",
        "        for container in house_containers:\n",
        "            \n",
        "            # Price            \n",
        "            price = container.find_all('span')[3].text\n",
        "            if price == 'Contacte Anunciante':\n",
        "                price = container.find_all('span')[3].text\n",
        "                if price.find('/') != -1:\n",
        "                    price = price[0:price.find('/')-1]\n",
        "            if price.find('/') != -1:\n",
        "                price = price[0:price.find('/')-1]\n",
        "            \n",
        "            price_ = [int(price[s]) for s in range(0,len(price)) if price[s].isdigit()]\n",
        "            price = ''\n",
        "            for x in price_:\n",
        "                price = price+str(x)\n",
        "            prices.append(int(price))\n",
        "\n",
        "            # Zone\n",
        "            location = container.find_all('p', class_=\"searchPropertyLocation\")[0].text\n",
        "            location = location[7:location.find(',')]\n",
        "            zone.append(location)\n",
        "\n",
        "            # Title\n",
        "            name = container.find_all('span')[0].text\n",
        "            titles.append(name)\n",
        "\n",
        "            # Status\n",
        "            status = container.find_all('p')[5].text\n",
        "            condition.append(status)\n",
        "\n",
        "            # Area\n",
        "            m2 = container.find_all('p')[9].text\n",
        "            if m2 != '-':\n",
        "                m2 = m2.replace('\\xa0','')\n",
        "                m2 = float(\"\".join(itertools.takewhile(str.isdigit, m2)))\n",
        "                areas.append(m2)\n",
        "                \n",
        "            else:\n",
        "                m2 = container.find_all('p')[7].text\n",
        "                if m2 != '-':\n",
        "                    m2 = m2.replace('\\xa0','')\n",
        "                    m2 = float(\"\".join(itertools.takewhile(str.isdigit, m2)))\n",
        "                    areas.append(m2)\n",
        "                else:\n",
        "                    areas.append(m2)\n",
        "\n",
        "            # Creation date\n",
        "            date = pd.to_datetime(container.find_all('div', class_=\"searchPropertyDate\")[0].text[21:31])\n",
        "            created.append(date)\n",
        "\n",
        "            # Description\n",
        "            desc = container.find_all('p', class_=\"searchPropertyDescription\")[0].text[7:-6]\n",
        "            descriptions.append(desc)\n",
        "\n",
        "            # url\n",
        "            link = 'https://casa.sapo.pt/' + container.find_all('a')[0].get('href')[1:-6]\n",
        "            urls.append(link)\n",
        "\n",
        "            # image\n",
        "            img = str(container.find_all('img')[0])\n",
        "            img = img[img.find('data-original_2x=')+18:img.find('id=')-2]\n",
        "            thumbnails.append(img)\n",
        "    else:\n",
        "        break\n",
        "    \n",
        "    sleep(randint(1,2))\n",
        "    \n",
        "print('You scraped {} pages containing {} properties.'.format(n_pages, len(titles)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRA22m-jyQzN"
      },
      "source": [
        "cols = ['Title', 'Zone', 'Price', 'Size (m²)', 'Status', 'Description', 'Date', 'URL', 'Image']\n",
        "\n",
        "lisboa = pd.DataFrame({'Title': titles,\n",
        "                           'Price': prices,\n",
        "                           'Size (m²)': areas,\n",
        "                           'Zone': zone,\n",
        "                           'Date': created,\n",
        "                           'Status': condition,\n",
        "                           'Description': descriptions,\n",
        "                           'URL': urls,\n",
        "                           'Image': thumbnails})[cols]\n",
        "\n",
        "lisboa.to_excel('lisboa_raw.xls')\n",
        "\n",
        "# lisboa = pd.read_excel('lisboa_raw.xls')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVzeIocByR4F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}